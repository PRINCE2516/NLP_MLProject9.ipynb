# NLP_MLProject9.ipynb


ğŸ¬ Movie Review Sentiment Analysis â€“ Theoretical Explanation
1ï¸âƒ£ Problem Statement

The goal of this project is to automatically determine the sentiment of a movie review â€“ whether it is positive ğŸ‘ or negative ğŸ‘.

Sentiment analysis is a key task in Natural Language Processing (NLP).

The challenge is to convert textual data into a form that a computer can understand and analyze.

2ï¸âƒ£ Dataset Overview

The dataset used is NLTKâ€™s movie_reviews corpus.

It contains 2000 reviews:

1000 positive reviews

1000 negative reviews

Each review is a plain text file.

Positive reviews generally contain words like â€œamazingâ€, â€œoutstandingâ€, while negative reviews have words like â€œboringâ€, â€œterribleâ€.

3ï¸âƒ£ Text Preprocessing

Before analyzing text, it must be cleaned and processed:

a) Tokenization âœ‚ï¸

Breaking text into individual words or tokens.

For example: â€œI love this movieâ€ â†’ ["I", "love", "this", "movie"].

Tokens are the building blocks for NLP.

b) Stopword Removal âŒ

Stopwords are very common words like â€œthe, is, andâ€ that donâ€™t carry sentiment meaning.

Removing stopwords reduces noise and improves classifier performance.

c) Punctuation Removal

Symbols like . , ! ? do not add meaning in sentiment classification.

Removing them simplifies the dataset.

4ï¸âƒ£ Feature Extraction â€“ Bag-of-Words ğŸ‘œ

Bag-of-Words (BoW) is a method to represent text as numerical features.

Each document (review) is represented as a dictionary of words, where the key is the word and the value indicates presence or frequency.

Example: â€œI love this movieâ€ â†’ {â€œloveâ€:1, â€œmovieâ€:1}.

BoW ignores word order, but captures the important words that indicate sentiment.

5ï¸âƒ£ Word Frequency Analysis ğŸ“Š

Counting word occurrences gives insight into which words are most informative.

Words like â€œfilm, movie, good, badâ€ appear frequently.

Rare words often carry strong sentiment, e.g., â€œoutstandingâ€, â€œinsultingâ€.

Visualizations like histograms or log-log plots show distribution of word frequencies.

6ï¸âƒ£ Creating Labeled Features

Each review is converted into a feature dictionary using BoW.

Paired with its label:

Positive review â†’ "pos"

Negative review â†’ "neg"

These labeled features are used to train a classifier.

7ï¸âƒ£ Model Selection â€“ Naive Bayes Classifier ğŸ¯

Naive Bayes is a probabilistic classifier based on Bayesâ€™ theorem.

Assumes that the presence of each word is independent of others (naive assumption).

Despite its simplicity, Naive Bayes performs very well for text classification, especially with BoW features.

How it works:

Calculate the probability of a review being positive or negative based on the words it contains.

Assign the class with the highest probability.

8ï¸âƒ£ Training and Testing

Dataset is split into:

Training set: Used to teach the classifier patterns in words â†’ sentiment.

Test set: Used to evaluate how well the classifier generalizes to new, unseen reviews.

Typical results:

Training accuracy: ~98% (classifier learns the training data well)

Test accuracy: ~71% (realistic performance on unseen data)

9ï¸âƒ£ Most Informative Words ğŸ”

After training, the classifier identifies words that are strong indicators of sentiment.

Examples:

Positive: â€œoutstandingâ€, â€œastoundingâ€, â€œfascinationâ€

Negative: â€œinsultingâ€, â€œludicrousâ€, â€œuninvolvingâ€

These words carry the strongest sentiment signals.

ğŸ”Ÿ Key Insights

BoW + Naive Bayes is simple yet effective for sentiment classification.

Stopword removal significantly improves accuracy.

The model captures sentiment using presence of important words, ignoring word order.

Frequent words are not always informative; rare words can carry strong sentiment signals.

Future Enhancements ğŸš€

Use TF-IDF weighting instead of simple presence to reduce impact of frequent neutral words.

Experiment with bigram or trigram models to capture word sequences.

Apply more advanced models like Logistic Regression, SVM, LSTM, or BERT for improved accuracy.
